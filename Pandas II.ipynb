{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0hmDzY7WFY7y"
   },
   "source": "# Pandas II"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "phjvXexXFY76"
   },
   "source": "<img src=\"https://i1.wp.com/numfocus.org/wp-content/uploads/2016/07/pandas-logo-300.png?fit=300%2C300&ssl=1\" width=\"200\">"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiPdtfMVFY7_"
   },
   "source": "In our previous notebook we saw the internals of pandas and that pandas is comprised of 3 objects: \n1. Index\n1. Series\n1. DataFrame  \n\nWe also saw that all 3 objects wrap a numpy array inside them. We saw how to query these objects, how they can leverage the power of numpy universal function and finished with how we can concatenate these objects together."
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Q6CrOnnFY8E"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gw5KSLLdFY8S"
   },
   "source": "## Merges\n***\nWe saw we can \"stitch\" together DataFrames and Series using the pandas concat method and the append methods which works like we are operating on a list.\n\n\nBut, in many cases we would like to perform smarter merges between dataframes. There is an entire universe and probably and entire course one can teach on databases merges and joins, but we will keep the discussion simple, mainly showcasing the more common operation and showing those who wish to take the longer path it's beginning.\n\nThis get us into the world of `pd.merge` We will talk about 4 of the merge types it supply for us:\n1. __Inner__ \n1. __Outer__ \n1. __Left__\n1. __Right__"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2TZDy6sFY8U"
   },
   "source": "Throughout this section we will work with the following toy dataset:"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qIQnCzDbFY8W"
   },
   "outputs": [],
   "source": "df_left = pd.DataFrame(data={\n    'Student' : ['michael', 'kobe', 'larry', 'james', 'kevin'],\n    'Age' : [23, 24, 33, 23, 35]\n})\n\ndf_right = pd.DataFrame(data={\n    'Student' : ['michael', 'larry', 'kevin', 'stephen'],\n    'Courses' : ['Machine Learning', 'Deep Learning', 'Statistics', 'Cloud Computing']\n})"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HlwvPG6FY8g",
    "outputId": "cda071ec-35d4-4246-8e47-80a6c638ba71"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "df_left"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-JTD25eFZBi",
    "outputId": "e7e04451-6a9a-455c-d333-57528e362e0a"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "df_right"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKLX2pCPFZCJ"
   },
   "source": "### Inner \nThe default merge is the inner merge. This will look for a common column, which will serve as the merge by column (each value in this column is a key), and will return the intersection between the keys. Any key which is not on one of the tables will be ignored."
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ov-uuZ9sFZCL",
    "outputId": "b9942dc2-56b3-4641-e3b4-d79823197c2b"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "pd.merge(df_left, df_right)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aBUz7cpyFZCU"
   },
   "source": "### Outer\nIf the inner join returned the intersection between the key, than the outer merge returns the union between the keys, filling up nan where a value is missing.  \nTo specify the merge type we used the `how` parameter."
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pn-epumdFZCW",
    "outputId": "30adfaf1-8c79-43ee-c232-afb087b526f9"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": "pd.merge(df_left, df_right, how='outer')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKsNZYyFFZCe"
   },
   "source": "### Left\nAs one might expect the left merge return all the keys in the left dataframe filling in nan where values are missing"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZbV2O-dpFZCf",
    "outputId": "1c59fd9f-ac56-42fb-affb-6011d34e6dfa"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": "pd.merge(df_left, df_right, how='left')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJ1eXtkwFZCj"
   },
   "source": "### Right\nAnd you guessed it the right merge returns all the keys in the right table filling in nan where values are missing"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HOgqTw14FZCl"
   },
   "outputs": [],
   "source": "pd.merge(df_left, df_right, how='right')"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWmZqg2IFZCo"
   },
   "source": "In the above example we used the fact that we add a single column which was in both of the datasets.  \nThere are cases where the is more than one joined columns, and there are times where the columns we wish to merge on have different names. All of these problems can be solved by passing in the correct parameters to the merge method which you will query in the next exercise."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UfaNBkcfFZCp"
   },
   "source": "***\n## Exercise\n***"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qx_xZEQQFZCr",
    "outputId": "f5c63492-6b2a-4cdf-e754-a49ab254df13"
   },
   "outputs": [],
   "source": "df_names = pd.DataFrame(data={\n    'Product' : ['shampoo', 'soap', 'tooth paste', 'toilet paper', 'sponage']\n}, index = ['412', '333', '525', '111', '212'])\n\ndf_prices = pd.DataFrame(data={\n    'price' : [12, 15, 13, 17, 26]\n}, index = ['412', '333', '525', '111', '555'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7VPvraDeFZCz"
   },
   "source": "__Find out the price of each product using a merge operation, keep all products but discard irrelevant prices. (keep the result in a new dataframe)__  \nRead the merge docs to find out how to merge on the index"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atHlsWuRFZC0",
    "outputId": "e998b025-7aaf-4daf-b8de-91d1fd0af5f1"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "# Your code starts here\ns = df_names.join(df_prices,how=\"inner\")\ns\n# Your code ends here\n"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AWOASN-wFZC4",
    "outputId": "2f382fcf-c099-4084-b8e9-adcb0ff9b79c"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "df_super_aisle = pd.DataFrame({\n    'type' : ['shampoo', 'soap', 'tooth paste', 'toilet paper', 'sponage'],\n    'aisle' : [0, 0, 12, 15, 9]\n})\ndf_super_aisle"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "owr43L5FFZDC"
   },
   "source": "__Create a dataframe holding both the product price and the aisle it should be in the supermarket__"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gPvdxFvmFZDE",
    "outputId": "09b6f995-9575-455d-bbdb-6238a03e4139"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "# Your code starts here\ns = df_names.join(df_prices,how=\"left\")\n\nres = pd.merge(s,df_super_aisle,left_on = \"Product\",right_on = \"type\")\nres\n# Your code ends here"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lClP2JHXFZDQ",
    "outputId": "0bf337e2-3c6d-4eda-cc1f-0f457b65ac68"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "df_super_owner = pd.DataFrame({\n    'owner' : ['Dave', 'Sean', 'Ashely', 'Gal'],\n    'aisle' : [0, 12, 15, 9]\n})\ndf_super_owner"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NA99T6A1FZDU"
   },
   "source": "__Find out who is in charge for each product(type - shampoo, soap, etc..)__"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1XDeYxV9FZDU",
    "outputId": "8f9bbb5e-5365-424c-ab99-5e570c1d0472"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "res = pd.merge(df_super_owner,df_super_aisle, on = \"aisle\",how = \"right\")\n# Your code starts here\n# Your code ends here\nres"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o5ElhM6JFZDX"
   },
   "source": "## Aggregation & Grouping\nWe have seen that numpy provides a handful of aggregation functions we can use the get statistics of our arrays.  \nWe can harvest those function in pandas as well."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gYtgIpAsFZDY"
   },
   "source": "### Series Aggregation\nAggregation over a series works the same as it does on 1d numpy array :"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jXgI26pvFZDf",
    "outputId": "5ed2daa1-5551-4772-a05b-90cc8c9ed1ce"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": "np.random.seed(1234)\ns = pd.Series(np.random.randint(low=10, high=100, size=10))\ns"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hLauRRekFZDh",
    "outputId": "583af3f8-fb87-4103-e141-23de43c4ae50"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": "s.mean(), s.std(), s.max()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "phYI3fw8FZDl"
   },
   "source": "### Dataframes aggregation\nAs one might expect the aggregation over a dataframe will default into preforming columns wise aggregation:"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_eFlZynpFZDm",
    "outputId": "73ca0c33-a4fe-4588-fc98-9953a57ddd3c"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": "np.random.seed(4321)\ndf = pd.DataFrame(np.random.randint(low=10, high=80, size=(12, 4)), columns=['A', 'B', 'C', 'D'])\ndf"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvtfdVLaFZDp",
    "outputId": "d379ad4a-f3c5-4dd0-c105-5c53ab4f3a55"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": "df.mean()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJvnznrsFZDx"
   },
   "source": "As we can see we get back the mean over each column (the return value is a series).  \nAnd as one might suspect, pandas aggregation functions give us the opportunity to supply a axis parameter which will enable us to choose the aggregation axis."
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTW2o8LMFZDy",
    "outputId": "9b8bf20f-b932-488c-a1f8-a8b61f01fe45"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": "df.mean(axis=1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PxRhIEg_FZD1"
   },
   "source": "Again the returned type is a Series, notice that the index of the series is derived by the index of the axis on which we operate. The column index if we operate on axis 0 and the row index if we operate on the 1 axis.  \nWhile this is nice, in many cases when we work on dataframes we which to preform our aggregation of groups inside the dataframes. For example we could have a **gender** column and we would like to aggregate over the Female and Male separately.  \n\n### Group By\nHere is an example:"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2aw1L5oFZD2",
    "outputId": "959c7760-0fb6-41a8-c49b-b1e5b6f38518"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "np.random.seed(10)\n\nkeys = np.random.choice(list('ABC'), size=12)\nf_value = np.random.randint(low=0, high=50, size=12)\ns_value = np.random.randint(low=0, high=50, size=12)\n\ndf = pd.DataFrame({\n    'keys' : keys,\n    'f_value' : f_value,\n    's_value' : s_value\n})\n\ndf"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6aLXfn1SFZD4"
   },
   "source": "So imagine that we would like the mean value of `f_value` and `s_value` but we would like to get the statistics for each key separately. Here's how we do it:"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "svJQR60pFZD5",
    "outputId": "917dc477-f79d-448a-84f6-1378365f6459"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "df.groupby('keys').mean()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zTfn-6RxFZD8"
   },
   "source": "If your wondering why the `keys` value is written there, that's because an index could have a name. (example on scratchpad)."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZt-gr-ZFZD9"
   },
   "source": "Ok, so what just happened here:  \n### Split - Apply - Combine\nThe process we just noticed is referred to as __split $\\rightarrow$ apply $\\rightarrow$ combine__\n1. __Split__ - We split our dataframes in to different groups of dataframes.\n1. __Apply__ - We apply some aggregation over each group separately.\n1. __Combine__ - We combine all the results in to one unified data frame.\n\nHere is in illustration:"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xcgBaoKGFZEB"
   },
   "source": "<img src='attachment:image.png' width=\"600\">"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYQvlY4HFZEB"
   },
   "source": "Another example:"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rdo7OBrdFZEC",
    "outputId": "9cf5088f-4419-4265-cd66-67aeb1ab8a35"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": "df.groupby('keys').count()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9OA3d95OFZEE"
   },
   "source": "#### Group by\nWhen we perform the group by operation we get back a gropuby object:"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tdr5MBlpFZEE",
    "outputId": "68113c6e-833b-4226-dd88-718245e2ae54"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": "g = df.groupby('keys')\ntype(g)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxY-RnkkFZEG"
   },
   "source": "__Notice__ : One might expect that after the groupby call we would get back each group as a separate dataframe. This is not the case. The groupby mechanism is intended to work as part of the split-aggregation-combine workflow and in practice it does so by creating 'views' into the original dataframe. This is intended to save space and time that will require to create all of these groups as a stand alone dataframes. \n\nLet's dive deeper - Instead of using one of pandas built in apply methods we will write our own and play with it."
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VY7TPmEFFZEI",
    "outputId": "50536e20-1b87-41d4-df0e-66a471dc40fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "   f_value keys  s_value\n2        0    A       36\n3       42    A       14\n5       36    A       13\n8       47    A       25\n--------------------\n   f_value keys  s_value\n2        0    A       36\n3       42    A       14\n5       36    A       13\n8       47    A       25\n--------------------\n    f_value keys  s_value\n0         8    B       33\n1         9    B        8\n4        40    B       49\n6        16    B        5\n7        36    B       13\n9        11    B       13\n10       24    B       28\n--------------------\n    f_value keys  s_value\n11       43    C       22\n--------------------\n********************\nEmpty DataFrame\nColumns: []\nIndex: []\n"
    }
   ],
   "source": "def my_apply(df):\n    print (df)\n    print('-' * 20)\n    return None\n    \nresult = df.groupby('keys').apply(my_apply)\nprint('*' * 20)\nprint(result)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-Lm7_iuFZEL"
   },
   "source": "You might ask yourself why the 'A' group printed twice? I know I asked myself this question. The answer is that pandas run the first group twice, in order to analyze whether it could use some internal optimization or not, it ignores the first value returned. Be aware in case you are trying to do something which will change on every iteration (if you save an outside count of some sort).\n\nNow, let's see we can emulate the mean behavior using the apply method:"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ODUy8HriFZEM"
   },
   "outputs": [],
   "source": "def my_apply(df):\n    return df.mean()\n\nresult = df.groupby('keys').apply(my_apply)\nresult == df.groupby('keys').mean()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qQTr4baiFZEO"
   },
   "source": "This open the doors to creating your own customs aggregation:"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CrrFEVJyFZEP"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "def range_size(df):\n    return df.max() - df.min()\n\ndf.groupby('keys').apply(range_size)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVCohs_DFZES"
   },
   "source": "Let's say I only want aggregation over one of the columns. I have a couple of ways I can tackle that :  \n1. Work only on the specific column\n1. Extract the column at the end\n"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gwMME850FZET"
   },
   "outputs": [],
   "source": "df.groupby('keys')['s_value'].mean()"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0fUkNIBCFZEW"
   },
   "outputs": [],
   "source": "df.groupby('keys')['s_value'].mean() == df.groupby('keys').mean()['s_value']"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WCbOzKC-FZEa"
   },
   "source": "Unless you have a very good reason you should go for the first option and not the second. In the second options your computing information you don't need. Which means you are wasting time and resources. Don't be that person."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tG83YsPNFZEb"
   },
   "source": "__Summing up:__  \nThe split-apply-combine flow enabled by the `groupby` method is a very convenient way of aggregating over specific groups in the data. You can use pandas built in aggregation function such as (`min`, `max`, `mean` etc..) or build your own apply function which can be as complex as possible."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpzPTkmpFZEc"
   },
   "source": "### apply\nIt's worth noting the apply method is not restricted to the groupby operation. The apply method in pandas allow you to iterate either columns or rows and execute a piece of code on each column / row. If you use apply on a Series you simply iterate over values.  \nHere is an example of applying over a series:"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wUbFIRd1FZEd",
    "outputId": "68e5c2ed-0db9-4067-d678-32b6dd3ab9cd"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "def lower_case(letter):\n    return letter.lower()\n\ndf['keys'].apply(lower_case)\ndf"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BV5KYi-0FZEg"
   },
   "source": "Here is an example of applying over a dataframe:"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s1Sekn2lFZEh",
    "outputId": "799209e6-6f03-47d2-8eba-52e415b28e24"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "def concat_row_to_str(row):\n    s = ''\n    for v in row.values:\n        s += str(v) \n        \n    return s\n    \ndf.apply(concat_row_to_str, axis=1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "swJaQ60xFZEj"
   },
   "source": "***\n## Exercise \n***"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHX4CDh8FZEj"
   },
   "outputs": [],
   "source": "from sklearn import datasets\nwine_dataset = datasets.load_wine()"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xha8ltx7FZEn",
    "outputId": "9e0027bc-6623-4772-9e86-35429e281462"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "X = wine_dataset.data\ny = wine_dataset.target\n\ndf = pd.DataFrame(X, columns=wine_dataset.feature_names)\ndf = pd.concat([df, pd.Series(y, name='quality')], axis=1)\n\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SrhzNup-FZEp"
   },
   "source": "__Find the mean `alcohol` level in each `quality` group__"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ogo54YyFZEq",
    "outputId": "2d7502f9-513f-43d6-b3da-b328f569a3fd"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "# Your code starts here\n# Your code ends here\ndf[['quality','alcohol']].groupby('quality').mean()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PEMojAndFZEu"
   },
   "source": "__Find the average `quality` and the count of wines with `magnesium` bigger then 100 and smaller than 100__  \nMeaning you should display the average quality and the number of wines used to compute this average.  \nLook in the docs of pandas [agg function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html) for how to perform more than one aggregation after the groupby command.\n* You can change the original dataframe"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "56eCXPNIFZEu",
    "outputId": "d326615e-3543-4f4a-8e32-82608966eed5"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "# Your code starts here\ntable1 = df[df['magnesium']>100]\ntable2 = df[df['magnesium']<=100]\ntable1[\"quality\"].mean()\ntable2[\"quality\"]\n#df.loc[df['magnesium']>100,'magnesium_100']='more'\n#df.loc[df['magnesium']<=100,'magnesium_100']='less'\n#df[['magnesium_100', 'quality']].groupby('magnesium_100').mean()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "31MTgTtsFZEx"
   },
   "source": "__Find the mean and standard deviation of the `ash` values in the following groups__:\n1. all wines with flavanoids of above 3\n1. all wines with flavanoids of between 2 and 3\n1. all wines with flavanoids of below 2  \n\n\n- You can change the original dataframe"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQ1f4V_cFZEy",
    "outputId": "b2a9d59b-f18e-441e-8166-2000e4181466"
   },
   "outputs": [
    {
     "data": {},
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "# Your code starts here \ndf.loc[df['flavanoids']>3, 'flavanoids_class']='above 3'\ndf.loc[df['flavanoids']<2, 'flavanoids_class']='below 2'\ndf.loc[(df['flavanoids']<=3)&(df['flavanoids']>=2), 'flavanoids_class']='btween 2&3'\n\ns = df.groupby('flavanoids_class')[\"ash\"].mean()\nd = df.groupby('flavanoids_class')[\"ash\"].std()\nq = pd.concat([s,d],axis=1)\nq.columns= [\"mean\",\"std\"]\nq"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kiuAvjrFZE5"
   },
   "source": "# References\n\n- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) A thorough tour into Numpy. \n- [How to use the Split-Apply-Combine strategy in Pandas groupby](https://towardsdatascience.com/how-to-use-the-split-apply-combine-strategy-in-pandas-groupby-29e0eb44b62e) An excellent blog summarizing the split apply combine flow."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UpzUw2kFZE6"
   },
   "source": "# Colophon\nThis notebook was written by __Yoav Orlev__ and is part of the __intro to Python for data science__ course at IDC Herzliya.\n\nThis work is licensed under a CC BY-NC-SA 4.0 International License."
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pandas II.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
